# Migrate Applications

This section addresses migrating applications by converting application manifests from the source format that were exported to `ocp-manifests/namespaces` folder to the target kubernetes manifests that will be saved into `clusterconfigs/namespaces`.

While it is possible to script out the entire workload migration, we recommend handling this migration one namespace at a time. This allows you to verify the workloads in each namespace are running before moving on to the next one. Also you can selectively migrate the workloads by prioritizing them, resolving any dependencies and by making sure that unnecessary workloads are not migrated.

We will be using [Shifter tool](https://github.com/garybowers/shifter) to convert the manifests from [OpenShift format to Kubernetes format](https://github.com/garybowers/shifter#openshift-to-kubernetes-converter). Download the latest release of Shifter [from here](https://github.com/garybowers/shifter/releases) onto your linux box.

```
wget https://github.com/garybowers/shifter/releases/download/v0.11/shifter-linux-amd64
mv shifter-linux-amd64 shifter
chmod +x shifter
```
Also set the PATH to access shifter or move it to a place where the PATH is already set.

## Converting Manifests

* Select a namespace you want to migrate. You can select one from the list of namespaces for which the manifests were exported previously `ls ocp-manifests/namespaces/`

```
NAMESPACE=<<selected namespace>>
```

* Set output location for the resultant target manifests. This can be set to `clusterconfigs/namespaces/$NAMESPACE` folder if you want to use ACM to deploy these manifests.

```
OUTPUT_LOCATION=clusterconfigs/namespaces/$NAMESPACE
```

* Find the list of manifests exported from the OpenShift cluster 

```
INPUT_LOCATION=ocp-manifests/namespaces/$NAMESPACE
ls $INPUT_LOCATION
```

* Convert each OpenShift DeploymentConfig listed to Kubernetes Deployments using Shifter. The script below creates a deployment for each manifest it the output folder with the same name as the openshift deployment configuration but ending in `deployment.yaml`.

```
for dc in $(ls $INPUT_LOCATION/*-dc.yaml); do
    echo "converting DC: " $dc
    OUTPUT_FILE=$(echo $dc | sed -e 's/ocp-manifests/clusterconfigs/' -e 's/-dc/-deployment/' -);
    shifter convert -i yaml -t yaml -f $dc -o $OUTPUT_FILE;
done
```

* Copy any Deployments, Services and ConfigMaps. There is no need to convert these manifests.

```
cp $INPUT_LOCATION/*-deployment.yaml $OUTPUT_LOCATION
cp $INPUT_LOCATION/*-service.yaml $OUTPUT_LOCATION
cp $INPUT_LOCATION/*-cm.yaml $OUTPUT_LOCATION
```

* If you have exported secrets earlier, copy any secrets to the target location. Again no conversion needed in this case
```
cp $INPUT_LOCATION/*-secret.yaml $OUTPUT_LOCATION
```

* Convert OpenShift Routes to Kubernetes Ingress Objects

```
for route in $(ls $INPUT_LOCATION/*-route.yaml); do
    echo "Converting Route: " $route
    OUTPUT_FILE=$(echo $route | sed -e 's/ocp-manifests/clusterconfigs/' -e 's/-route/-ingress/' -);
    shifter convert -i yaml -t yaml -f $route -o $OUTPUT_FILE;
done
```

* If you have Persistent volumes to migrate, follow the procedure [here](WIP)

## Create Image Pull Secret 

The target cluster may require credentials to pull images from a container registry, if the registry is protected. In this section, we will see how to set up a secret to the container registry. We will use GCR as an example.

If you are using GCR as the registry, in the previous section you have seen how to [transfer images from OpenShift Internal Registry to GCR](./11.TransferApplicationImages.md). 

To provide access to your application deployment to pull images from this registry, create a service account that can pull images from the private registry, download the service account key and create a secret [as explained in this article](https://cloud.google.com/anthos/clusters/docs/aws/how-to/private-registry)

Once you download the key, create a secret in the target namespace with the key as shown below. Change the name of the secret, if you want to.

```
IMAGEPULLSECRET=gcr-secret
SERVICE_ACCOUNT_EMAIL=registry-sa@ocptogkedemoproject.iam.gserviceaccount.com 

kubectl create secret docker-registry $IMAGEPULLSECRET         --docker-server=gcr.io         --docker-username=_json_key         --docker-email=$SERVICE_ACCOUNT_EMAIL    --docker-password="$(cat key.json)" -n $NAMESPACE
```

## Update the deployments to point to Target Registry

* **If the images have been migrated** to [a different registry as part of migration](./11.TransferApplicationImages.md) from OpenShift Internal Registry, change the container image in the deployment to point to the target repository where the image is moved.

    A deployment in OpenShift would be referencing the image from internal registry using its [internal DNS name](https://kubernetes.io/docs/concepts/services-networking/service/#dns). In an OpenShift 4.x cluster this is always set to `image-registry.openshift-image-registry.svc:5000` as the registry service is named `image-registry` and runs in namespace `openshift-image-registry` by the openshift image registry operator. You can verify this by looking at the deployment. Set appropriate values for OpenShift `INTERNAL_REGISTRY_URL` and the `TARGET_REPO`.

```
INTERNAL_REGISTRY_URL=image-registry.openshift-image-registry.svc:5000
TARGET_REPO="gcr.io\/<<your-project>>"
```

**NOTE**:
* OpenShift uses specific image shaid in the deployment. Sometimes the target cluster doesnt recognize the specific image id and you may see error like this `Failed to fetch "sha256:8537f5b2211c69e07b9a855ba661ee5b218486aab4b41ed4f0d199e22ce34e30" from request`. In order to prevent this issue we will replace shaid with the `latest` tag. 

Run the following code snippet to replace the images in the deployment to point to the target repository and to the `latest` image as some images may have shaid that may not work. 

```
for deployment in $(ls $OUTPUT_LOCATION/*-deployment.yaml); do
    CHANGED_IMAGE=$(cat $deployment | yq e '.spec.template.spec.containers[].image' - | sed -e 's/'$INTERNAL_REGISTRY_URL'/'$TARGET_REPO'/' -e 's/@sha256:.*$/:latest/') ;
    cat $deployment \
    | yq e '.spec.template.spec.containers[].image |= "'$CHANGED_IMAGE'"' - \
    > $deployment.new;
    mv $deployment $deployment.bak
    mv $deployment.new $deployment
done

```

**NOTE** 
* Sometimes the image names are inconsistent between the imagestream public repository name and the one used in the deployment configurations. Verify the resultant deployment reference to the image and the image actually migrated to the target repository to make sure they are the same. If required, you may have to change the image name



## Apply Security Constraints to the Deployments


* Generate secure deployments, depending on the policies that apply to your namespace. Refer the [ACM Security Polices that were set up earlier](./8.SetupRestrictedConstraints.md). If your application runs with [restricted security constraints](./policies/restricted/restricted_constraints.yaml), we have to update deployments with security settings that meet those constraints. In the example below, we will apply specific USERID, FSGROUPID, drop capabilities.
Also, if we are pulling from a registry that requires credentials, that were configured in the previous steps, we will configure the secret as the image pull secret. Set the values of USERID, FSGROUPID and IMAGEPULLSECRET to appropriate values before running the script below.

```
USERID=1001
FSGROUPID=1101

for deployment in $(ls $OUTPUT_LOCATION/*-deployment.yaml); do
    echo "Generating Restricted Deployment for : " $deployment;
    cat $deployment \
    | yq e 'del(.spec.template.spec.securityContext)'  - \
    | yq e '.spec.template.spec.securityContext.runAsUser |= '$USERID'' - \
    | yq e '.spec.template.spec.securityContext.fsGroup |= '$FSGROUPID'' - \
    | yq e '.spec.template.spec.containers[].securityContext.capabilities.drop |= ["KILL","MKNOD","SYS_CHROOT"]' - \
    | yq e '.spec.template.spec.initContainers[].securityContext.capabilities.drop |= ["KILL","MKNOD","SYS_CHROOT"]' - \
    | yq e '.spec.template.spec.imagePullSecrets[0].name |= "'$IMAGEPULLSECRET'"' - \
    > $deployment.new;
    mv $deployment $deployment.bak
    mv $deployment.new $deployment
done;
```

Verify the deployments generated to make sure they are good with the changes.


## Applying Application Manifests to the Target Cluster

* Connect to the Target GKE Cluster, if you are not already connected

```
kubectx <<target cluster>>
```

* Apply ConfigMaps to the target cluster

```
for cm in $(ls $OUTPUT_LOCATION/*-cm.yaml); do
    kubectl -n $NAMESPACE apply -f $cm
done
```

* Apply secrets, if any, to the target cluster. **NOTE** Generally, you may create these secrets directly instead of importing from the source cluster.

```
for secret in $(ls $OUTPUT_LOCATION/*-secret.yaml); do
    kubectl -n $NAMESPACE apply -f $secret
done
```

* Apply the deployments to the target cluster
```
for deployment in $(ls $OUTPUT_LOCATION/*-deployment.yaml); do
    kubectl -n $NAMESPACE apply -f $deployment
done
```
Verify that the pods are running. You can look at `kubectl get events -n $NAMESPACE --sort-by=lastTimestamp` to debug in case of any issues.


You have two choices while creating services. For the services that are externally reachable you can either use Kubernetes Ingress or you can use LoadBalancer type service.

### Applying Kubernetes Ingress

* Apply the services to the target cluster
```
for svc in $(ls $OUTPUT_LOCATION/*-service.yaml); do
    kubectl -n $NAMESPACE apply -f $svc
done
```

* Apply the kubernetes ingresses to the target cluster
```
for ingress in $(ls $OUTPUT_LOCATION/*-ingress.yaml); do
    kubectl -n $NAMESPACE apply -f $ingress
done
```

### Applying Loadbalancer type service

In this case identify the services that need to be reachable from outside the cluster. Only those externally accessible services should be changes to `LoadBalancer` type.

* For each service that needs to be exposed run after replacing the service filename with appropriate value.

```
SERVICE=$OUTPUT_LOCATION/<<servicefilename>>

cat $SERVICE \
| yq e '.spec.type |= "LoadBalancer"' - | kubectl -n $NAMESPACE apply -f -
```

Once you create all services verify that these LoadBalancer type services are assigned ExternalIP by running `kubectl get svc -n $NAMESPACE`. 

**NOTE** 
* It may take a few minutes for the External_IP to be assigned.
* K8S Services configured in OpenShift usually don't use port 80 , but 8080, by default for http traffic. So if you are exposing services as LoadBalancer type without changing ports, verify the port number and access the service on the port exposed.

Access the application by using any of the above to ingress mechanisms using External IP.









