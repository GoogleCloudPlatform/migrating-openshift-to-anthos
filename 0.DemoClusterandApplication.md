# Standing up an OpenShift cluster and a sample application for testing

If you are trying out these scripts yourself and want to standup an OpenShift cluster to test the migration scripts, this document helps. In a realistic scenario, you already have an OpenShift cluster with some running applications to migrate. So this section is only meant to try out the scripts.

## Stand up an OpenShift Cluster

We will leverage GCP as the deployment platform for OpenShift. This documentation will not serve to be a replacement for Red Hat's documentation, but should assist as high-level guidance.  This process will create a  OpenShift cluster with the following profile.

* Three (3) Master Nodes with one (1) associated Cloud NAT Gateway
* Three (3) Worker Nodes with one (1) associated Cloud NAT Gateway
* Two (2) Public IPs with two (2) Load balancers and associated health checks
* One (1) Public DNS Zone within Cloud DNS


**1.** Create your GCP Project (if needed)

   **Important:** The GCP project name _must be_ globally unique across all GCP projects.
   In this example, we will assign the variable `GCP_PROJECT` for ease of use.

```shell script
#Set a environment variable for reuse that is globally unique
export GCP_PROJECT=openshift-cluster-20210422

#Create a GCP project, if needed
gcloud projects create ${GCP_PROJECT} --name="My OpenShift Cluster"
#Set the config to the target project
gcloud config set project ${GCP_PROJECT}
```

You must now assign this project to an appropriate Billing Account [details here](https://cloud.google.com/billing/docs/how-to/modify-project#to_change_the_projects_account_do_the_following)

**2.** Enable the necessary API services in GCP

```shell script
gcloud services enable compute.googleapis.com \
          cloudapis.googleapis.com \
          cloudresourcemanager.googleapis.com \
          dns.googleapis.com \
          iamcredentials.googleapis.com \
          iam.googleapis.com \
          servicemanagement.googleapis.com \
          serviceusage.googleapis.com \
          storage-api.googleapis.com \
          storage-component.googleapis.com
```

**3.** Configuring DNS _within_ your GCP Project

   The Red Hat OpenShift installer as describe below will look within the project for Cloud DNS entries.([Full details here](https://developers.redhat.com/developer-sandbox)). 
   For this example, we will use `openshift.example` as the `DOMAIN_NAME` variable.

   * Create a public zone within `${GCP_PROJECT}` In the following example, 
   we will assume the root level domain is `example.com` and the sub-domain is `ocp`

```shell script
gcloud dns managed-zones create ocp \
    --description="OCP DNS Public Zone" \
    --dns-name="ocp.example.com" \
    --visibility=public
```
**Important:** Cloud DNS creates NS and SOA records for you automatically when you create the zone. Do not change the name of your zone's NS record, and do not change the list of name servers that Cloud DNS selects for your zone.

   * After successful DNS Zone creation, you will need to review the nameserver (NS) records and update your _registrar_ records with the Name Servers populated from the Cloud DNS public zone created in step 1 above.  Guidance for these steps is out of scope of this material.  However, if you are using Google Domain, you might review the following:
   * [How to switch to custom name servers](https://support.google.com/domains/answer/3290309?hl=en)
   * [Migrating to Cloud DNS in the GCP documentation](https://cloud.google.com/dns/docs/migrating)

**4.** Review [GCP account limits and quotas](https://docs.openshift.com/container-platform/4.7/installing/installing_gcp/installing-gcp-account.html#installation-gcp-limits_installing-gcp-account) to ensure GCP resources are available 


**5.** Create a service account, role assignment, and  in GCP for the installer and add to the appropriate GCP roles.

   The following is a broad approach to security for the installer.  This is intended for demonstration purposes only. Be sure to follow all 
corporate security standards where they may apply (more insights on role requirements [here](https://docs.openshift.com/container-platform/4.7/installing/installing_gcp/installing-gcp-account.html#installation-gcp-permissions_installing-gcp-account)).
   
```shell script
# Set an environment variable for ease of use
SERVICE_ACCOUNT_ID=ocp-cluster-sa

#Create the Service Account
gcloud iam service-accounts create ${SERVICE_ACCOUNT_ID} \
    --description="Service account for OCP Installer and cluster management" \
    --display-name="OpenShift Service Account"

#Asign the correct role binding
gcloud projects add-iam-policy-binding ${GCP_PROJECT} \
    --member="serviceAccount:${SERVICE_ACCOUNT_ID}@${GCP_PROJECT}.iam.gserviceaccount.com" \
    --role="roles/owner"

# Export the key (WARNING: Protect this key)    
gcloud iam service-accounts keys create ocp-cluster-sa-key \
    --iam-account=${SERVICE_ACCOUNT_ID}@${GCP_PROJECT}.iam.gserviceaccount.com
```

**6.** Install OCP with the easy, infrastructure provided installer 

   * Review [prerequisites](https://docs.openshift.com/container-platform/4.7/installing/installing_gcp/installing-gcp-default.html#prerequisites) and 
[Supported Regions](https://docs.openshift.com/container-platform/4.7/installing/installing_gcp/installing-gcp-account.html#installation-gcp-regions_installing-gcp-account)
   
   * Access the [OpenShift on GCP with installer-provisioned infrastructure](https://cloud.redhat.com/openshift/install/gcp/installer-provisioned). If you have a Red Hat account, log in with your credentials. If you do not, create an account, and you will have access to 60 day trial.

   * Download the appropriate binaries 
   
**Important:** Keep this page available.  During the installation process, you will be prompted for the `Pull Secret`.

*Recommendation:* The installation process will take about 45min.  It is best to run the installer from a stable workstation or server. It is _not_ recommended running the installer from a remote session that may be reset due to lack of activity (e.g. Cloud Shell).


* Set the `GOOGLE_APPLICATION_CREDENTIALS` to the path of the security key we exported as `ocp-cluster-sa-key` in the previous step.

```shell script
export GOOGLE_APPLICATION_CREDENTIALS=ocp-cluster-sa-key

# Run the installer and follow the prompts.
./openshift-install create cluster --dir=${PWD}/install --log-level=info
```

The following may show during installation:

* _SSH Public Key:_ For demo purposes, you may select 'None'
* _Platform:_ Select 'gcp'
* _INFO Credentials:_ This is the value set in the above ${GOOGLE_APPLICATION_CREDENTIALS} variable
* _Project ID:_ Should default to your GCP Project 
* _Region:_ Select from one of the supported GCP Regions
* _Base Domain:_ Must match DNS Cloud Public Zone entry created within the GCP target Project
* _Cluster Name:_ This is the name of the cluster.  This will be used when generating sub-domains.
* _Pull Secret:_ Copy into your clip board (from Pull Secret) as proved in the OpenShift Cluster Manager at https://cloud.redhat.com/

You now have a basic install of OpenShift on GCP.  `kubeadmin` password will be displayed at the end of the installation
process embedded within the log messages.  If you closed the terminal or lost the password, you may locate it in the path
defined above (e.g., `${PWD}/install/auth/kubeadmin-password`)

**7.** The default install only provisions the `kubeadmin` user.  To create addtional users, the easiest approach is to add an OAuth ID provider.  
You may add that in the `Global configuration -> OAuth details -> Identity providers: Add`

For illustrative purposes, the following is a represenative of [Google as the Identity Provider](https://docs.openshift.com/container-platform/4.7/authentication/identity_providers/configuring-google-identity-provider.html).
```yaml
  identityProviders:
   - google:
       clientID: >-
         XXXXXXX.apps.googleusercontent.com
       clientSecret:
         name: google-client-secret-xxxx
       hostedDomain: example.com
     mappingMethod: claim
     name: googleidp
     type: Google
```

## Deploy a sample 2 tiered application on OpenShift 

* Create a new project on the OpenShift cluster

```
oc new-project demo
```
* Clone a git repo that has sample application to be deployed

```
git clone https://github.com/sclorg/nodejs-ex
```

* Deploy a two-tier application using an openshift template from this repository

```
oc new-app -f nodejs-ex/openshift/templates/nodejs-mongodb.json -n demo
```
You will see output that looks like below

```

     * With parameters:
        * Name=nodejs-mongodb-example
        * Namespace=openshift
        * Version of NodeJS Image=12
        * Version of MongoDB Image=3.6
        * Memory Limit=512Mi
        * Memory Limit (MongoDB)=512Mi
        * Git Repository URL=https://github.com/sclorg/nodejs-ex.git
        * Git Reference=
        * Context Directory=
        * Application Hostname=
        * GitHub Webhook Secret=oHSyoUbfl3mj1Lf0bE7Qjfq1pGn0l7WQ6u4tUUL3 # generated
        * Generic Webhook Secret=J6A5g0JuYN3Ij0ETgn6wOeuR1pr8VXLsHaOdRiuY # generated
        * Database Service Name=mongodb
        * MongoDB Username=user22W # generated
        * MongoDB Password=eQap2qcytpHkuHi7 # generated
        * Database Name=sampledb
        * Database Administrator Password=tL6cNAQ4rBGijOUR # generated
        * Custom NPM Mirror URL=
--> Creating resources ...
    secret "nodejs-mongodb-example" created
    service "nodejs-mongodb-example" created
    route.route.openshift.io "nodejs-mongodb-example" created
    imagestream.image.openshift.io "nodejs-mongodb-example" created
    buildconfig.build.openshift.io "nodejs-mongodb-example" created
    deploymentconfig.apps.openshift.io "nodejs-mongodb-example" created
    service "mongodb" created
    deploymentconfig.apps.openshift.io "mongodb" created
--> Success
    Access your application via route 'nodejs-mongodb-example-demo.apps.ocp46.ocp.jduke.me' 
    Build scheduled, use 'oc logs -f buildconfig/nodejs-mongodb-example' to track its progress
.
    Run 'oc status' to view your app.
```

* The sample nodejs application with a backend mongodb database will be built and deployed on the openshift cluster. In a couple of minutes you should see two pods running when you check with `oc get pods -n demo` as shown below:

```
$ oc status
In project Demo App for Migration (demo) on server https://api.ocp46.ocp.jduke.me:6443
svc/mongodb - 172.30.179.80:27017
  dc/mongodb deploys openshift/mongodb:3.6 
    deployment #1 deployed 30 minutes ago - 1 pod
http://nodejs-mongodb-example-demo.apps.ocp46.ocp.jduke.me (svc/nodejs-mongodb-example)
  dc/nodejs-mongodb-example deploys istag/nodejs-mongodb-example:latest <-
    bc/nodejs-mongodb-example source builds https://github.com/sclorg/nodejs-ex.git on openshi
ft/nodejs:12 
    deployment #1 deployed 29 minutes ago - 1 pod
View details with 'oc describe <resource>/<name>' or list resources with 'oc get all'.

$ oc get po
NAME                              READY   STATUS      RESTARTS   AGE
mongodb-1-deploy                  0/1     Completed   0          2m11s
mongodb-1-sbsnz                   1/1     Running     0          2m9s
nodejs-mongodb-example-1-build    0/1     Completed   0          2m12s
nodejs-mongodb-example-1-deploy   0/1     Completed   0          70s
nodejs-mongodb-example-1-rrr48    1/1     Running     0          67s

$ oc get svc
NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
mongodb                  ClusterIP   172.30.179.80   <none>        27017/TCP   4m17s
nodejs-mongodb-example   ClusterIP   172.30.246.74   <none>        8080/TCP    4m17s

$ oc get route
NAME                     HOST/PORT                                             PATH   SERVICES
                 PORT    TERMINATION   WILDCARD
nodejs-mongodb-example   nodejs-mongodb-example-demo.apps.ocp46.ocp.jduke.me          nodejs-m
ongodb-example   <all>                 None

```
We can migrate this application while testing the scripts.

## Create a cluster viewer role

Once your applications are deployed, you can limit the risk of accidental changes to the source OpenShift cluster by using a user account that is limited to the `cluster-reader` Role.  Below is an example of binding a user to the `cluster-reader` ClusterRole vis a Cluster Role Binding.

```shell script
oc create clusterrolebinding my-cluster-reader --clusterrole=cluster-reader --user= someuser@ocp.example.com
```

**Important:** By default, the `Cluster-Reader` does not have access to read secrets. If you need to export secrets, you will need a higher-prevledged account.
